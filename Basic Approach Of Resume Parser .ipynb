{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from pdfminer.pdfinterp import PDFResourceManager,PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from pdfminer.pdfinterp import PDFResourceManager,PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from io import BytesIO\n",
    "import re\n",
    "from spacy.matcher import Matcher\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "class Resume_parser:\n",
    "    def __init__(self,path):\n",
    "        self.path=path\n",
    "        self.text=self.pdf_to_text()\n",
    "        self.name=self.extract_name()\n",
    "        self.mobile=self.extract_mobile_number()\n",
    "        self.email=self.extract_email()\n",
    "        self.skills=self.extract_skills()\n",
    "        self.education=self.extract_education()\n",
    "        self.link=self.extract_project_link()\n",
    "    def pdf_to_text(self):\n",
    "        manager = PDFResourceManager()\n",
    "        retstr = BytesIO()\n",
    "        layout = LAParams(all_texts=True)\n",
    "        device = TextConverter(manager, retstr, laparams=layout)\n",
    "        filepath = open(self.path, 'rb')\n",
    "        interpreter = PDFPageInterpreter(manager, device)\n",
    "        for page in PDFPage.get_pages(filepath, check_extractable=True):\n",
    "            interpreter.process_page(page)\n",
    "            text = retstr.getvalue()\n",
    "        filepath.close()\n",
    "        device.close()\n",
    "        retstr.close()\n",
    "        return text.decode()\n",
    "    def extract_name(self):\n",
    "        #Load Pretrain data\n",
    "        nlp = spacy.load('en_core_web_sm')\n",
    "        # initialize matcher with a vocab\n",
    "        matcher = Matcher(nlp.vocab)\n",
    "        nlp_text = nlp(self.text)\n",
    "        # First name and Last name are always Proper Nouns\n",
    "        pattern = [{'POS': 'PROPN'}, {'POS': 'PROPN'}]\n",
    "        matcher.add(\"NAME\",None,pattern)\n",
    "        matches = matcher(nlp_text)\n",
    "        for match_id, start, end in matches:\n",
    "            span = nlp_text[start:end]\n",
    "            return span.text\n",
    "    def extract_mobile_number(self):\n",
    "        phone = re.findall(re.compile(r'(?:(?:\\+?([1-9]|[0-9][0-9]|[0-9][0-9][0-9])\\s*(?:[.-]\\s*)?)?(?:\\(\\s*([2-9]1[02-9]|[2-9][02-8]1|[2-9][02-8][02-9])\\s*\\)|([0-9][1-9]|[0-9]1[02-9]|[2-9][02-8]1|[2-9][02-8][02-9]))\\s*(?:[.-]\\s*)?)?([2-9]1[02-9]|[2-9][02-9]1|[2-9][02-9]{2})\\s*(?:[.-]\\s*)?([0-9]{7})(?:\\s*(?:#|x\\.?|ext\\.?|extension)\\s*(\\d+))?'),self.text)\n",
    "        if phone:\n",
    "            number = ''.join(phone[0])\n",
    "            if len(number) > 10:\n",
    "                return '+' + number \n",
    "            else:\n",
    "                return number\n",
    "    def extract_email(self):\n",
    "        email = re.findall(\"([^@|\\s]+@[^@]+\\.[^@|\\s]+)\",self.text)\n",
    "        if email:\n",
    "            try:\n",
    "                return email[0].split()[0].strip(';')\n",
    "            except IndexError:\n",
    "                return None\n",
    "    def extract_skills(self):\n",
    "        nlp = spacy.load('en_core_web_sm')\n",
    "        nlp_text = nlp(self.text)\n",
    "        noun_chunks = nlp_text.noun_chunks\n",
    "\n",
    "        # removing stop words and implementing word tokenization\n",
    "        tokens = [token.text for token in nlp_text if not token.is_stop]\n",
    "\n",
    "        # extract values\n",
    "        skills =[\"node.js\",\"deep learning\",'php,'\"mongodb\",\"nodejs\",\"redis\",\"ajax\"]\n",
    "\n",
    "        skillset = []\n",
    "\n",
    "        # check for one-grams (example: python)\n",
    "        for token in tokens:\n",
    "            if token.lower() in skills:\n",
    "                skillset.append(token) \n",
    "        # check for bi-grams and tri-grams (example: machine learning)\n",
    "        for token in noun_chunks:\n",
    "            token = token.text.lower().strip()\n",
    "            if token in skills:\n",
    "                skillset.append(token)\n",
    "\n",
    "        return [i.capitalize() for i in set([i.lower() for i in skillset])]\n",
    "    def extract_education(self):\n",
    "        nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "        # Grad all general stop words\n",
    "        STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "        # Education Degrees\n",
    "        EDUCATION = [\n",
    "\n",
    "                'BE','B.E.', 'B.E', 'BS', 'B.S', \n",
    "                'ME', 'M.E', 'M.E.', 'MS', 'M.S', \n",
    "                'BTECH', 'B.TECH', 'M.TECH', 'MTECH', \n",
    "                'SSC', 'HSC', 'CBSE', 'ICSE', 'X', 'XII'\n",
    "            ]\n",
    "        nlp_text = nlp(self.text)\n",
    "\n",
    "        # Sentence Tokenizer\n",
    "        nlp_text = [sent.string.strip() for sent in nlp_text.sents]\n",
    "\n",
    "        edu = {}\n",
    "        # Extract education degree\n",
    "        for index, text in enumerate(nlp_text):\n",
    "            for tex in text.split():\n",
    "                # Replace all special symbols\n",
    "                tex = re.sub(r'[?|$|.|!|,|(|)]', r'', tex)\n",
    "                if tex.upper() in EDUCATION and tex not in STOPWORDS:\n",
    "                    if index+1<len(nlp_text):\n",
    "                        edu[tex] = text + nlp_text[index+1]\n",
    "                    else:\n",
    "                        edu[tex] = text + nlp_text[index]\n",
    "        # Extract year\n",
    "        education = []\n",
    "        for key in edu.keys():\n",
    "            year = re.search(re.compile(r'(((20|19)(\\d{2})\\s*-\\s*(20|19)(\\d{2})))|((20|19)(\\d{2}))'), edu[key])\n",
    "            if year:\n",
    "                education.append((key, ''.join(year[0])))\n",
    "            else:\n",
    "                education.append(key)\n",
    "        return education\n",
    "    \n",
    "    def extract_project_link(self):\n",
    "        nlp = spacy.load('en_core_web_sm')\n",
    "        Link=[\"https\",\"http\"]\n",
    "        nlp_text=nlp(self.text)\n",
    "        nlp_text=[sent.string.strip() for sent in nlp_text.sents]\n",
    "        project_link=[]\n",
    "        for link in nlp_text:\n",
    "            for protocol in link.split(':'):\n",
    "                if protocol.lower() in Link:\n",
    "                    project_link.append(link)\n",
    "        return project_link\n",
    "rae=Resume_parser(\"./Resume (2).pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mahajan Sahil\n",
      "sahilmahajanstar@gmail.com\n",
      "+919657859876\n",
      "['Node.js', 'Ajax', 'Redis', 'Deep learning']\n",
      "[('BE', '2017 - 2021'), ('XII', '2017'), ('X', '2015'), 'HSC']\n",
      "https://www.linkedin.com/in/timbuchalka/?originalSubdomain=au\n",
      "https://drive.google.com/drive/folders/1Gm3NBturTH4VEPoxKSZIirdl5OBsEckX?usp=shar\n",
      "https://github.com/sahilmahajanstar/FarmWorkDiary/blob/master/Farm.py\n",
      "https://drive.google.com/drive/folders/1C4p62GqJlFmpRVup2dWa3J4aI9q47JwC?usp=sha\n"
     ]
    }
   ],
   "source": [
    "print(rae.name)\n",
    "print(rae.email)\n",
    "print(rae.mobile)\n",
    "print(rae.skills)\n",
    "print(rae.education)\n",
    "for link in rae.link:\n",
    "    print(link)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
